{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 3\n",
    "## Efficiently finding optimal policies in MABs\n",
    "\n",
    "In this assignment, we will work with Multi Armed Bandit environments, and try to find the best policies using different strategies to minimize the total regret.\n",
    "\n",
    "The aim of this exercise is to code agents capable of understanding the underlying probability distributions of the environment and finding the most optimal actions as early as possible.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary stuff\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# if you want to use envs from Gym, import it\n",
    "# import gym, gym_bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple **2-armed Bernoulli** bandit.\n",
    "\n",
    "If you want a cleaner code, you can implement Bandits using `class` in Python.\n",
    "\n",
    "We have included sample code for this in `bandits.py` which you can take/import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bernoulli_bandit():\n",
    "    def __init__(self, n):\n",
    "        self.N = n\n",
    "        self.mean = np.random.random()\n",
    "        self.probs = np.random.normal(self.mean, 1, (n,))\n",
    "        self.probs = np.random.rand(n)\n",
    "        self.optimal = np.max(self.probs)\n",
    "        self.regret = 0.0\n",
    "        \n",
    "    def evaluate_choice(self, k):\n",
    "        assert 1 <= k <= self.N\n",
    "        reward = np.random.binomial(1, self.probs[k-1])\n",
    "        self.regret += self.optimal - reward\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian_bandit():\n",
    "    def __init__(self, n):\n",
    "        self.N = n\n",
    "        self.means = np.random.rand(n)\n",
    "        self.variances = np.sqrt(means)*(np.random.rand(n)/2 + 0.5)\n",
    "        self.optimal = np.max(self.means)\n",
    "        self.regret = 0.0\n",
    "\n",
    "    def evaluate_choice(self, k):\n",
    "        assert 1 <= k <= self.N\n",
    "        reward = np.random.uniform(self.means[k-1], self.variances[k-1]) \n",
    "        self.regret += np.random.uniform(self.optimal, self.variances[k-1]) - reward\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make a template for testing different strategies.\n",
    "\n",
    "Feel free to modify this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy function takes in the environment function, number of actions, and a selector function\n",
    "# it also takes in the number of episodes to run the strategy for (higher episodes = more accurate Q values)\n",
    "\n",
    "def test_strategy(env, n_actions, selector, n_episodes = 1000):\n",
    "    \n",
    "    # initialize Q and N to 0s\n",
    "    Q = np.zeros(n_actions)\n",
    "    N = np.zeros(n_actions)\n",
    "\n",
    "    # loop for n_episodes\n",
    "    for e in tqdm(range(n_episodes)):\n",
    "        \n",
    "        # selector function takes in current Q and returns an action\n",
    "        # modify the selector function according to the strategy\n",
    "        action = selector(Q)\n",
    "\n",
    "        # get the reward from the environment\n",
    "        reward = env(action)\n",
    "\n",
    "        # update N and Q\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action])/N[action]\n",
    "\n",
    "    # return the best action\n",
    "    return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the simplest selector using pure-exploration strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selector returns a random action\n",
    "random_selector = lambda Q : np.random.randint(len(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strategy(mab_2_env, 2, random_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it returns the optimal action. Let's check if that's indeed true.\n",
    "\n",
    "We can do that by revealing the actual `probs` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80499935, 0.65065254])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our pure exploration strategy does indeed return the optimal action for this Bernoulli bandit. \n",
    "\n",
    "You can try generating new bandits with different `probs` and try out the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this in place, here's what you have to do -\n",
    "\n",
    "Recall that, the regret $\\mathcal{T}$ is given by,\n",
    "\n",
    "$$\\mathcal{T}=\\sum  _{e=1} ^{E} \\mathbb{E} \\left[ v_* - q_* \\left( A_e \\right) \\right]$$\n",
    "\n",
    "We can only calculate it when we have the $v_*$ and $q_*$ functions known beforehand. Since we are making the MDPs from scratch, that's not an issue for us right now.\n",
    "\n",
    "But remember, in real-life problems, these functions are not known. Hence we must be aware of multiple policy finding strategies and try the one which gives best results fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 0\n",
    "\n",
    "Implement the calculation of the total regret $\\mathcal{T}$ for your strategy.\n",
    "\n",
    "To do this, you will need to store the rewards obtained each episode. Modify the `strategy` function accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate total regret\n",
    "\n",
    "def regret(rewards, probs):\n",
    "\n",
    "    # implement the regret function here\n",
    "    return len(rewards)*np.max(probs) - sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 1\n",
    "\n",
    "Now, let's implement some other selection strategies and compare their regret with the simple exploration strategy.\n",
    "\n",
    "Note that some of these strategies involve hyperparameter(s) which need to be manually adjusted. You have to play around with the values and see which one gives you best results.\n",
    "\n",
    "This is known as \"hyperparameter tuning\" and is quite commonly done while working with complex models (including neural networks). Personally, you should try out some natural values (including the ones given in the book) along with some extreme values where it is easy to manually verify the correctness of your strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def epsilon_greedy(Q, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(len(Q))\n",
    "    else:\n",
    "        return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponentially decaying epsilon greedy strategy\n",
    "\n",
    "def exponentially_decaying_epsilon_greedy(Q, epsilon, episode):\n",
    "    \n",
    "    # implement the exponentially decaying epsilon greedy strategy here\n",
    "    epsilon *= episode\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(1, len(Q) + 1), epsilon\n",
    "    return np.argmax(Q) + 1, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax action selection strategy\n",
    "\n",
    "def softmax_strategy(Q, temperature):\n",
    "    \n",
    "    # implement the softmax strategy here\n",
    "    choice_probabilities = [np.exp(val/temperature) for val in Q]\n",
    "    choice_probabilities = choice_probabilities/sum(choice_probabilities)\n",
    "\n",
    "    return np.random.choice(range(1, len(Q)+1), p=choice_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper confidence bound strategy\n",
    "from math import log, log10\n",
    "\n",
    "def ucb(Q, N, c):\n",
    "    \n",
    "    # implement the ucb strategy here\n",
    "    s = sum(N)\n",
    "    choice = np.argmax([Q[i] + np.sqrt(log(s)/N[i]) if N[i] >= 1 else Q[i] for i in range(len(Q))]) + 1\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thompson sampling strategy\n",
    "\n",
    "def thompson_sampling(Q, N, a, b):\n",
    "    samples = np.random.normal(loc=Q, scale = a/(np.sqrt(N) + b))\n",
    "    choice = np.argmax(samples) + 1\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 2\n",
    "\n",
    "Run each strategy for 2-armed bandit environment and compare the total regrets.\n",
    "\n",
    "You can also try plotting the regret vs episode graph and check if it matches the expected result from Grokking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = Bernoulli_bandit(10)\n",
    "bandit.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_epsilon_greedy(epsilon=0.1, gamma=0.9, num_episodes = 10000):\n",
    "    global bandit\n",
    "    bandit.regret = 0\n",
    "    gamma_power = gamma\n",
    "    Q = np.zeros(bandit.N)\n",
    "    N = np.zeros(bandit.N)\n",
    "    for episode in range(num_episodes):\n",
    "        choice = epsilon_greedy(Q, epsilon)\n",
    "        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)\n",
    "        N[choice-1] += 1\n",
    "        gamma_power *= gamma\n",
    "    return np.argmax(Q) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_decaying_epsilon_greedy(epsilon, gamma=0.9, num_episodes = 10000):\n",
    "    global bandit\n",
    "    min_to_max_ratio = 0.01\n",
    "    max_epsilon = epsilon\n",
    "    min_epsilon = min_to_max_ratio*epsilon\n",
    "    decay_rate = 0.05\n",
    "    decay_episodes = int(num_episodes*decay_rate)\n",
    "    remaining_episodes = num_episodes - decay_episodes\n",
    "    epsilons = 0.01\n",
    "    epsilons /= np.logspace(log10(min_to_max_ratio), 0, decay_episodes)\n",
    "    epsilons *= max_epsilon - min_epsilon\n",
    "    epsilons += min_epsilon\n",
    "    epsilons = np.pad(epsilons, (0, remaining_episodes), mode='constant', constant_values=min_epsilon)\n",
    "    bandit.regret = 0\n",
    "    gamma_power = gamma\n",
    "    Q = np.zeros(bandit.N)\n",
    "    N = np.zeros(bandit.N)\n",
    "    for episode in range(num_episodes):\n",
    "        choice = epsilon_greedy(Q, epsilons[episode])\n",
    "        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)\n",
    "        N[choice-1] += 1\n",
    "        gamma_power *= gamma\n",
    "        \n",
    "    return np.argmax(Q) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_softmax(temperature, gamma=0.9, num_episodes = 10000):\n",
    "    global bandit\n",
    "    bandit.regret = 0\n",
    "    gamma_power = gamma\n",
    "    Q = np.zeros(bandit.N)\n",
    "    N = np.zeros(bandit.N)\n",
    "    for episode in range(num_episodes):\n",
    "        choice = softmax_strategy(Q, temperature)\n",
    "        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)\n",
    "        N[choice-1] += 1\n",
    "        gamma_power *= gamma\n",
    "    return np.argmax(Q) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_ucb(c, gamma=0.9, num_episodes = 10000):\n",
    "    global bandit\n",
    "    bandit.regret = 0\n",
    "    gamma_power = gamma\n",
    "    Q = np.zeros(bandit.N)\n",
    "    N = np.ones(bandit.N)\n",
    "    for episode in range(num_episodes):\n",
    "        choice = ucb(Q, N, c)\n",
    "        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)\n",
    "        N[choice-1] += 1\n",
    "        gamma_power *= gamma\n",
    "    return np.argmax(Q) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_thompson_sampling(alpha=1, beta=0, gamma = 0.9, num_episodes = 10000):\n",
    "    global bandit\n",
    "    bandit.regret = 0\n",
    "    gamma_power = gamma\n",
    "    Q = np.zeros(bandit.N)\n",
    "    N = np.zeros(bandit.N)\n",
    "    for episode in range(num_episodes):\n",
    "        choice = thompson_sampling(Q, N, alpha, beta)\n",
    "        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)\n",
    "        N[choice-1] += 1\n",
    "        gamma_power *= gamma\n",
    "    return np.argmax(Q) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values = np.linspace(0, 1, 101)\n",
    "regrets = []\n",
    "choices = []\n",
    "for epsilon in epsilon_values:\n",
    "    optimal_choice = implement_decaying_epsilon_greedy(epsilon)\n",
    "    bandit.regret = 0\n",
    "    print(optimal_choice)\n",
    "    for episode in range(1000):\n",
    "        bandit.evaluate_choice(optimal_choice)\n",
    "    regrets.append(bandit.regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Regret')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.title('Exponentially Decaying Epsilon Greedy Policy')\n",
    "plt.scatter(epsilons, regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilons = np.linspace(0, 1, 101)\n",
    "epsilon_values = [0.1]\n",
    "values = []\n",
    "rewards = []\n",
    "results = []\n",
    "for i in range(50):\n",
    "    band = Bernoulli_bandit(n)\n",
    "    epsilon = epsilon_values[0]\n",
    "    n = 2\n",
    "    band.regret = 0\n",
    "    gamma = 0.9\n",
    "    gamma_power = 0.9\n",
    "    Q = np.zeros(n)\n",
    "    for episode in range(10000):\n",
    "        choice = epsilon_greedy(Q, epsilon)\n",
    "        Q[choice-1] += gamma_power*band.evaluate_choice(choice)\n",
    "        gamma_power *= gamma\n",
    "    rewards.append(band.regret)\n",
    "    results.append(abs(band.probs[0] - band.probs[1])/band.regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.linspace(0, 1, 101)\n",
    "regrets = []\n",
    "for epsilon in epsilons:\n",
    "    bandit.regret = 0\n",
    "    gamma = 0.9\n",
    "    gamma_power = 0.9\n",
    "    Q = np.zeros(n)\n",
    "    N  = np.zeros(n)\n",
    "    for episode in range(10000):\n",
    "        choice = epsilon_greedy(Q, epsilon)\n",
    "        Q[choice-1] = (Q[choice-1]*N[choice-1] + gamma*bandit.evaluate_choice(choice))/(N[choice-1] + 1)\n",
    "        N[choice-1] += 1\n",
    "        gamma_power *= gamma\n",
    "    bandit.regret = 0\n",
    "    choice = np.argmax(Q) + 1\n",
    "    for episode in range(10000):\n",
    "        bandit.evaluate_choice(choice)\n",
    "    regrets.append(bandit.regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Regret')\n",
    "plt.title('Epsilon Greedy Policy')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.scatter(epsilons, regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_values = np.linspace(0, 1, 101)[1:]\n",
    "regrets = []\n",
    "choices = []\n",
    "for temp in temperature_values:\n",
    "    optimal_choice = implement_softmax(temp)\n",
    "    bandit.regret = 0\n",
    "    print(optimal_choice)\n",
    "    for episode in range(1000):\n",
    "        bandit.evaluate_choice(optimal_choice)\n",
    "    regrets.append(bandit.regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Regret')\n",
    "plt.title('Softmax Policy')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.scatter(temperature_values, regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = np.linspace(0, 1, 101)\n",
    "regrets = []\n",
    "choices = []\n",
    "for c in c_values:\n",
    "    optimal_choice = implement_ucb(c)\n",
    "    bandit.regret = 0\n",
    "    print(optimal_choice)\n",
    "    for episode in range(1000):\n",
    "        bandit.evaluate_choice(optimal_choice)\n",
    "    regrets.append(bandit.regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Regret')\n",
    "plt.title('UCB Policy')\n",
    "plt.xlabel('c')\n",
    "plt.scatter(c_values, regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 3\n",
    "\n",
    "The 2-armed bandits might be too simple for us to actually see substantial difference in the regret of these strategies. \n",
    "\n",
    "Let's now create a more complicated bandit environment and replicate our results on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a 10-armed Gaussian bandit. \n",
    "\n",
    "As required, it will have possible actions and each action will generate a reward sampled from a Gaussian distribution.\n",
    "\n",
    "Hence, each \"arm\" will have a randomly generated $\\mu$ and $\\sigma$, and the rewards will be generated with probabilities following the $\\mathcal{N}(\\mu, \\sigma^2)$ distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 arm gaussian bandit\n",
    "n = 10\n",
    "means = 10*np.random.rand(n)\n",
    "\n",
    "variances = np.sqrt(means)*np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = Gaussian_bandit(n)\n",
    "bandit.means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 4\n",
    "\n",
    "Test the different strategies on the 10-armed gaussian bandit and verify your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values = np.linspace(0, 1, 101)\n",
    "regrets = []\n",
    "choices = []\n",
    "for epsilon in epsilon_values:\n",
    "    optimal_choice = implement_epsilon_greedy(epsilon)\n",
    "    bandit.regret = 0\n",
    "    print(optimal_choice)\n",
    "    for episode in range(1000):\n",
    "        bandit.evaluate_choice(optimal_choice)\n",
    "    regrets.append(bandit.regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Regret')\n",
    "plt.title('Epsilon Greedy Policy')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.scatter(epsilons, regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values = np.linspace(0, 1, 101)\n",
    "regrets = []\n",
    "choices = []\n",
    "for epsilon in epsilon_values:\n",
    "    optimal_choice = implement_decaying_epsilon_greedy(epsilon)\n",
    "    bandit.regret = 0\n",
    "    print(optimal_choice)\n",
    "    for episode in range(1000):\n",
    "        bandit.evaluate_choice(optimal_choice)\n",
    "    regrets.append(bandit.regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Regret')\n",
    "plt.title('Decaying Epsilon Greedy Policy')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.scatter(epsilons, regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_values = np.linspace(0, 1, 101)[1:]\n",
    "regrets = []\n",
    "choices = []\n",
    "for temp in temperature_values:\n",
    "    optimal_choice = implement_softmax(temp)\n",
    "    bandit.regret = 0\n",
    "    print(optimal_choice)\n",
    "    for episode in range(1000):\n",
    "        bandit.evaluate_choice(optimal_choice)\n",
    "    regrets.append(bandit.regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Regret')\n",
    "plt.title('Softmax Policy')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.scatter(temperature_values, regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = np.linspace(0, 1, 101)\n",
    "regrets = []\n",
    "choices = []\n",
    "for c in c_values:\n",
    "    optimal_choice = implement_ucb(c)\n",
    "    bandit.regret = 0\n",
    "    print(optimal_choice)\n",
    "    for episode in range(1000):\n",
    "        bandit.evaluate_choice(optimal_choice)\n",
    "    regrets.append(bandit.regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Regret')\n",
    "plt.title('UCB Policy')\n",
    "plt.xlabel('c')\n",
    "plt.scatter(c_values, regrets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
